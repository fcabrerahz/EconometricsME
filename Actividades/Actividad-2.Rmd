---
title: "Actividad 2"
output:
  pdf_document: default
  'pdf_document: default': default
date: ''
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

### Ejercicio 1. Propiedades Finitas.

a) Suopnga que $E[e|X]=0$ y $var[e|X]=\Omega$ pruebe que:

$$E[\hat{\beta}|X]=\beta;$$

$$var[\hat{\beta}|X] = (X'X)^{-1}(X' \Omega X) (X'X)^{-1}$$

 
b) The parameter $\beta$ is esimtated by OLS $\hat{\beta}= (X'X)^{-1}X'Y$ and GLS $\tilde\beta=(X'\Omega^{-1}X)^{-1}X'\Omega^{-1}Y$. Let $\hat{e} = Y - X\hat{\beta}$ and $\tilde{e} = Y - X\tilde{\beta}$ denote the residuals. How would you compute $\hat{R}^2$ and $\tilde{R}^2$. If the error $e_i$ is truly heteroskedastic will $\hat{R}^2$ or $\tilde{R}^2$ be smaller?

c) Weighted Least Squares is a special case of GLS. Where $var(e_i|X) = \sigma^2 h(X)$ for $h(X)>0$. So the functional form of heteroskedasticity is known. Show that $var(e_i h_i^{-1/2})=\sigma^2$. Is WLS efficient?

d) Take the linear homoskedastic CEF $Y^* = X'\beta + e$; $E[e|X]=0$; $E[e^2|X]=\sigma^2$. Suppose that Y* is measured with error. Instead, we observe $Y=Y^*+u$ where u is a measurement error, $e$ and $u$ are independent and $E[u|X]=0$; $E[u^2|X]=\sigma^2_u (X)$. Describe the effect if any of this measurement error on variance calculation for $\hat\beta$.  

e) Muestre que $E[\hat{V}_{\hat\beta}^{HC0}|X]$ se encuentra sesgado bajo homoscedasticidad (vea BH 113). Junto con esto, muestre que $HC2$, bajo homoscedasticidad, es insesgado.

f) Siga el ejemplo en R abajo. Obtenga la proyección lineal de Y en X utlizando OLS. Encuentre los errores estándar robustos (hint: lmtest package) y muestre que $HC1<HC2<HC3$ 

```{r, echo=TRUE}

# generate heteroskedastic data 
X <- 1:1000
Y <- rnorm(n = 1000, mean = X, sd = 0.6 * X)

# plot the data
plot(x = X, y = Y, 
     pch = 19, 
     col = "steelblue", 
     cex = 0.8)
```

g) Utilize la base "cps09mar.txt" y siga el código provisto en R en BH p.91.Obtenga los errores HC0 a HC3. 
 - Modifique $h_{ii}$ para que tenga un valor cercano a .4 y obtenga de nuevo HC0 a HC3 ¿a qué conclusión llega?

h) Obtenga en forma matricial (similar a como hizo en g) los errores estándar en cluster utilizando la base de datos "DDK2011.txt".

i) Carga el conjunto de datos 'PublicSchools' desde la paqueteria 'sandwich'. Ajusta un modelo de regresión lineal simple usando OLS donde 'Expenditure' es la variable dependiente e 'Income' la variable independiente.

- Realice un diagnostico de la regresión (pueden usar funciones como plot()).
- Interprete los resultados de sus diagnósticos ¿Hay algún estado que influya desproporcionadamente en el modelo? ¿Cómo podría modificar el modelo para manejar estas influencias?

j) Carga el conjunto de datos 'Journals' de Stock y Watson (2007), que contiene información sobre suscripciones a revistas de economía en bibliotecas de EE. UU. desde la paqueteria 'AER'.

-  Genere un nuevo **data frame** que contenga solo las variables de interés: número de suscripciones (subs), precio de suscripción (price) y el precio por cita calculado (citeprice).
-  Transforme las variables subs y citeprice usando el logaritmo natural para su análisis.
-  Ajuste un modelo de regresión lineal usando OLS donde el logaritmo del número de suscripciones se explique por el logaritmo del precio por cita.
-  Realice un diagnóstico del modelo de regresión.
-  Interprete los resultados de los diagnósticos.
-  Reporte los errores estándar correctos para los coeficientes estimados.

k) En el contexto de una encuesta nacional sobre la satisfacción con los servicios de salud pública, se ha utilizado un diseño de muestreo por conglomerados. Los investigadores seleccionaron inicialmente ciertas áreas geográficas (unidades primarias de muestreo) y luego realizaron una selección aleatoria de hogares dentro de estas áreas para las entrevistas. Discuta por qué es importante utilizar errores estándar agrupados en la estimación de los coeficientes de un modelo de regresión que analice los determinantes de la satisfacción con los servicios de salud.



l) Con base en el ejemplo abajo, ejecute dos simulaciones con n=500 y 10,000 repeticiones que reproduzcan, en la medida de lo posible, los siguientes gráficos:

```{r, out.width='50%', echo=FALSE}
knitr::include_graphics(c("act2_1.jpg", "act2_2.jpg"))
```

*Example:* Here we create a matrix X  with two vectors (regressors $X_1$ and $X_2$) with median 50 and 100 each, and $cov(X_1,X_2)$ = 0.25

```{r, echo=TRUE, eval=FALSE}

library(MASS)
X <-rmvnorm(n, c(50, 100), sigma = cbind(c(10, 2.5), c(2.5, 10)))
summary (X)
plot(X[,1], X[,2]) 

```

Explique en qué factor aumenta la varianza de acuerdo con los parametros establecidos $n$, $\sigma^2$ y $\rho$ siguiendo la definición de $var[\hat{\beta}|X]$ en BH p.121.   


### Ejercicio 2. MLE (preliminar).

a) Show that plugging in the estimators $\beta_{mle}^2$ and $\hat{\sigma}^2_{mle}$ into: 

$$l_n(\beta, \sigma^2) = -{n \over 2}log(2\pi\sigma^2)-{1 \over {2\sigma^2}}\sum_{i=1}^n (Y_i - X'_i\beta)^2$$

We get the maximized log likelihood function (see BH p.142-143). How does it work as a measure o fit?

b)  Suponga que $y_i|\mathbf{x}_i\sim\mathcal{N}(m(\mathbf{x}_i,\mathbf{\beta}_0),\sigma_0^2)$, donde $m(\mathbf{x},\mathbf{\beta})$ es una función del vector de variables explicativas $\mathbf{x}$ y del vector de parámetros $\mathbf{\beta}$ de dimensión $(k\times 1)$. Entonces, $E(y_i|\mathbf{x}_i)=m(\mathbf{x}_i,\mathbf{\beta}_0)$ y $V(y_i|\mathbf{x}_i)=\sigma^2_0$.

- Escriba la función de log verosimilitud condicional para la observación $i$. Muestre que el estimador de máxima verosimilitud $\hat{\mathbf{\beta}}$ resuelve el problema de minimización $\min_\mathbf{\beta}\sum_i(y_i-m(\mathbf{x}_i,\mathbf{\beta}))^2$.
-  Sea $\mathbf{\theta}\equiv(\mathbf{\beta}'\;\sigma^2)'$ un vector de parámetros de dimensión $(k+1)\times 1$. Encuentre el vector score para la observación $i$. Muestre que $E(\mathbf{s}_i(\mathbf{\theta}_0)|\mathbf{x}_i)=\mathbf{0}$.
-  Usando las condiciones de primer orden, encuentre $\hat{\sigma}^2$ en términos de $\hat{\mathbf{\beta}}$.
-  Encuentre la matriz hesiana de la función de log verosimilitud con respecto a $\mathbf{\theta}$.

